{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Dyssynchrony Index\n",
    "We will be tackling a sequence classification problem with recurrent neural networks. We believe that the vectorcardiogram is a good predictor of the dyssynchrony index, so we will treat the VCG as a sequence of coordinates and feed it into a LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dimensions\n",
    "\n",
    "### *Input*\n",
    "Our input will be (simulated) vectorcardiograms generated by Chris Villongco using CMRG's Continuity.\n",
    "Our dataset consists of 608 simulated VCGs. These simulations have varying parameters, such as differing stimulus sites and conduction velocities, but are based on the same patient (patient 2, BiV2). We limit our dataset to only 608 examples in the interest of speeding up computational time, as we are more interested in showing proof of concept than obtaining a higher accuracy (of course, we will aim for higher accuracy later).\n",
    "\n",
    "Because we view the VCG as a sequence, each timestep can be viewed as one element in the sequence. For every time ```t```, we are given three inputs, the  ```(x, y, z)``` coordinates of the vector head.\n",
    "\n",
    "### *Output*\n",
    "We will be classifying each VCG based on the corresponding dyssynchrony index from that same simulation. The dyssynchrony index is a scalar value that theoretically ranges from 0 to 1, but we will only be concerned with the range of 0.5 to 1. This range will be further divided into 5 regular intervals, which will serve as our \"classes\". Specifically, these intervals will be ```[0.5, 0.6), [0.6, 0.7), [0.7, 0.8), [0.8, 0.9)```, and ```[0.9, 1.0]```. Simulations with dyssynchrony indices under 0.5 will be considered faulty and will be placed in the first interval, ```[0.5, 0.6)```. Also, note that simulations with dyssynchrony indices of 1.0 will be placed in the last interval, ```[0.9, 1.0]```. These intervals will be labeled as follows (intervals are 0-indexed):\n",
    "* ```0: [0.5, 0.6)```\n",
    "* ```1: [0.6, 0.7)```\n",
    "* ```2: [0.7, 0.8)```\n",
    "* ```3: [0.8, 0.9)```\n",
    "* ```4: [0.9, 1.0]```\n",
    "  \n",
    "For example, a VCG sequence with a corresponding dyssynchrony index of 0.78 will be placed in class ```\"2\"``` since it falls in the range of 0.7 and 0.8, the third interval.\n",
    "\n",
    "Thus, the output of the neural network will be a probability distribution signifying how likely a simulation with the given VCG will have a dyssynchrony index that falls under each of the five intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Wrapper\n",
    "We've created a class that provides a basic interface for handling the dataset. Specifically, the wrapper will do the following: \n",
    "* Read in the dataset from three specified ```.npy``` files (VCG, VCG lengths, target class)\n",
    "* Split the dataset into training, validation, and testing sets (the set sizes will be fixed for convenience)\n",
    "* Provide a ```next_batch``` function that will return a batch of specified size for a given set.\n",
    "\n",
    "We import the wrapper here. To instantiate, we specify the names of the NumPy files for the following:\n",
    "* ```vcg.npy```: VCG sequences (input)\n",
    "* ```vcg_length.npy```: VCG sequence lengths (passed as argument for ```sequence_length.npy``` parameter)\n",
    "* ```target.npy```: dyssynchrony indices (target output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import Patient\n",
    "\n",
    "# Initialize dataset iterator\n",
    "patient_dataset = Patient(\"dataset/vcg.npy\", \"dataset/vcg_length.npy\", \"dataset/target.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "### Network Dimensions\n",
    "We will define the dimensions of our data, as well as the initial hyperparameters of our neural network here. Note: these parameters have not been optimized, they are simply for proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 18 # 990 training examples / 1 batch of 55 = 18 total batches\n",
    "num_hidden = 200\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "num_steps = 130\n",
    "num_inputs = 3\n",
    "num_classes = 5\n",
    "epochs = 0\n",
    "\n",
    "\n",
    "# Where TensorFlow saves metadata for TensorBoard\n",
    "logs_path='Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Placeholders\n",
    "We define three placeholders. They are for the following:\n",
    "* VCG sequence input: ```[num_steps, batch_size, num_inputs]```\n",
    "* VCG sequence length: ```[batch_size]```\n",
    "* targets (not hot vectors): ```[batch_size]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# VCG input [130, None, 3]\n",
    "x = tf.placeholder(tf.float32, [num_steps, None, num_inputs])\n",
    "\n",
    "# VCG sequence lengths\n",
    "sequence_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Index of class the VCG should be categorized as\n",
    "y = tf.placeholder(tf.int64, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases\n",
    "The recurrent neural network creates an output at every timestep. Since this is a problem of sequence classification, we are only interested in the output produced at the last timestep, ```t=t_end```. We then apply a linear activation on it. The weights and biases are initialized with random values from a normal distribution, with a mean of 0.0 and a standard deviation of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# Define weights and biases\n",
    "\n",
    "# [100, 5]\n",
    "weights = tf.Variable(tf.random_uniform(\n",
    "        shape = [num_hidden, num_classes],\n",
    "        minval = (-1 / math.sqrt(num_hidden)),\n",
    "        maxval = (1 / math.sqrt(num_hidden))\n",
    "))\n",
    "# weights = tf.Variable(tf.random_normal(\n",
    "#         shape = [num_hidden, num_classes],\n",
    "#         ))\n",
    "# [5]\n",
    "biases = tf.Variable(tf.zeros([num_classes]))\n",
    "# biases = tf.Variable(tf.random_normal([num_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Cell\n",
    "Here we define what kind of recurrent neural network we will be using. We will be using a basic LSTM network with a default forget bias of 1.0, and ```tanh``` as the activation function. The ```BasicLSTMCell``` initializer function takes as parameters:\n",
    "* ```num_units```: The number of units in a LSTM cell.\n",
    "* ``` forget_bias```: float, the bias added to the forget gates.\n",
    "* ``` activation```: activation function of the inner states. Default is ```tanh```.\n",
    "* ``` state_is_tuple```: Accepted and returned states are 2-tuples of the c_state and m_state(???). Default is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import rnn_cell\n",
    "\n",
    "# Define a lstm cell with tensorflow\n",
    "cell = rnn_cell.BasicLSTMCell(\n",
    "    num_units=num_hidden,\n",
    "    forget_bias=5.0, \n",
    "    state_is_tuple=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Operation\n",
    "We will be using the ```tf.nn.dynamic_rnn``` function, instead of the ```tf.nn.rnn``` function, to get the output of the recurrent neural network. Unlike ```tf.nn.rnn```, ```tf.nn.dynamic_rnn``` takes in variable sequence lengths (it uses a ``tf.While`` loop to dynamically construct the computational graph). Also, it is faster (supposedly), despite the fact that ```tf.nn.rnn``` prebuilds the graph. The parameters are as follows:\n",
    "* ```cell```: an instance of RNN cell.\n",
    "* ```dtype```: (optional) The data type for the initial state and the expected output. \n",
    "* ```sequence_length```: (optional) An int32/64 vector of size [batch_size] specifying the length of each sequence.\n",
    "* ```inputs```: the RNN input, a single Tensor. The dimensions are [batch_size, sequence_length, num_inputs]\n",
    "* ```time_major```: Specifies that the max number of timesteps comes as the first dimensions, so the input placeholder must be of shape ```[max_time, batch_size, num_inputs]```. This requires us to permute the input matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs, states = tf.nn.dynamic_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=sequence_length,\n",
    "    inputs=x,\n",
    "    time_major=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We bring to attention the parameter ```time_major```, which we set to ```True```. This specifies that for our input placeholder, the ```max_time``` will be the *first* dimension, so it must have shape ```[max_time, batch_size, num_inputs]```. As a result, the output tensor would have shape ```[max_time, batch_size, num_hidden]```. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: (130, ?, 200)\n"
     ]
    }
   ],
   "source": [
    "# Shape of output tensor\n",
    "print \"Output tensor shape: \" + str(outputs.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is advantageous for two reasons:\n",
    "* We can easily access the *last* timestep by calling ```output[-1]``` (we are only concerned with the last timestep as this is a sequence classification problem).\n",
    "* Increases efficiency because it avoids transpositions at the beginning and end of the RNN calculation (https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/recurrent_neural_networks#dynamic_rnn)\n",
    "\n",
    "However, we must alter our input, because they come in shape ```[batch_size, max_time, num_input]```, as it was the more *intuitive* way of bundling our data. Specifically, we must permute the ```0th``` and ```1st``` dimension so that the shape will be ```[max_time, batch_size, num_inputs]```. We can do so by calling NumPy's ```np.swapaxes``` function on ```batch_x``` before passing it into the ```feed_dict```.\n",
    "\n",
    "Below is an example of permuting the ```0th``` and ```1st``` axes of the batch of VCGs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape before swap: (55, 130, 3)\n",
      "Input batch shape after swap: (130, 55, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize a dummy iterator for the purpose of this example\n",
    "dummy_dataset = Patient(\"dataset/vcg.npy\", \"dataset/vcg_length.npy\", \"dataset/target.npy\")\n",
    "\n",
    "# Grab the first batch\n",
    "dummy_x, dummy_length, dummy_target = dummy_dataset.train.next_batch()\n",
    "\n",
    "# Get the shape before permutation\n",
    "print \"Input batch shape before swap: \" + str(dummy_x.shape)\n",
    "\n",
    "# Permute the 0th and 1st axes\n",
    "dummy_x = np.swapaxes(dummy_x, 0, 1)\n",
    "print \"Input batch shape after swap: \" + str(dummy_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation\n",
    "After we get the outputs for every timestep, we extract the output for the *last* timestep and apply a linear activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output after linear activation: (?, 5)\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.matmul(outputs[-1], weights) + biases\n",
    "\n",
    "# Shape should be [batch_size, num_classes]\n",
    "print \"Shape of output after linear activation: \" + str(prediction.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "Up to this point, we are able to feed forward our input and have the neural network output its \"prediction\". Now we need to set up some key functions to allow for this network to be trained.\n",
    "\n",
    "### Cross Entropy Loss\n",
    "We will be using TensorFlow's ```tf.nn.sparse_softmax_cross_entropy_with_logits``` function which measures the probability error in discrete classification tasks in which the classes are *mutually exclusive*. Note that this operation expects unscaled logits and it performs softmax internally for efficiency. \n",
    "\n",
    "Its parameters are as follows:\n",
    "* logits: float32/64 with shape ```[batch_size, num_classes]```\n",
    "* labels: int32/64 with shape ```[batch_size]``` where each entry is a value between ```[0, num_classes)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of costs (before averaging): (?,)\n"
     ]
    }
   ],
   "source": [
    "# Calculate costs for each example\n",
    "costs = tf.nn.sparse_softmax_cross_entropy_with_logits(prediction, y)\n",
    "print \"Shape of costs (before averaging): \" + str(costs.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates the cost for each example in the batch. To find the batch cost, we average over all costs in the batch using ```tf.reduce_mean```.\n",
    "\n",
    "Its parameters are as follows:\n",
    "* input_tensor: The tensor to reduce. Should have numeric type.\n",
    "* axis: The dimensions to reduce. If None (the default), reduces all dimensions.\n",
    "* keep_dims: If true, retains reduced dimensions with length 1.\n",
    "* name: A name for the operation (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cost (scalar value): ()\n"
     ]
    }
   ],
   "source": [
    "# Cost function\n",
    "cost = tf.reduce_mean(costs)\n",
    "\n",
    "# Output is a scalar value\n",
    "print \"Shape of cost (scalar value): \" + str(cost.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "Once we define our cost function, we now know what we are trying to minimize. We will use a TensorFlow-defined operation that implements the Adam Algorithm, ```tf.train.AdamOptimizer``` with the following parameters:\n",
    "* learning_rate: A Tensor or a floating point value. The learning rate (default is 0.001)\n",
    "* beta1: A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates (default is 0.9).\n",
    "* beta2: A float value or a constant float tensor. The exponential decay rate for the 2nd moment estimates (default is 0.999).\n",
    "* epsilon: A small constant for numerical stability (default is 1e-08).\n",
    "* use_locking: If True use locks for update operations (default is False).\n",
    "* name: Optional name for the operations created when applying gradients. Defaults to \"Adam\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize TF optimizer\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "    learning_rate=learning_rate,\n",
    "    name=\"AdamOptimizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we initialize an optimizer, we can simply call the ```optimizer.minimize()``` function to both calculate the gradients and apply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training op\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Accuracy\n",
    "Instead of looking at the raw cost to determine the model's performance, we can simply calculate how accurately the model correctly predicted the right class that the VCG falls in. To extract the *class* that the model predicted, we simply call ```tf.argmax``` which returns the index with the largest across a specified axis of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy shape (scalar value): ()\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions with targets\n",
    "compare = tf.equal(tf.argmax(prediction, 1), y)\n",
    "\n",
    "# Cast booleans to ints\n",
    "count_correct = tf.reduce_mean(tf.cast(compare, tf.float32))\n",
    "accuracy = tf.mul(count_correct, 100)\n",
    "\n",
    "# Shape should be a single scalar value\n",
    "print \"Accuracy shape (scalar value): \" + str(accuracy.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

      "Testing accuracy (before): 20.909090.\n",
      "Epoch 0 of 0 loss: 27.1996885538\n",
      "Testing accuracy (After): 39.090912.\n"
     ]
    }
   ],
   "source": [
    "# Initialize all the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Grab validation set\n",
    "#     valid_x = patient_dataset.validate.vcg\n",
    "#     valid_length = patient_dataset.validate.vcg_length\n",
    "#     valid_y = patient_dataset.validate.target\n",
    "    \n",
    "    # Output accuracy for validation set\n",
    "    print \"Testing accuracy (before): %2f.\" % sess.run(accuracy, feed_dict={\n",
    "            x: np.swapaxes(test_x, 0, 1), \n",
    "            sequence_length: test_length, \n",
    "            y: test_y \n",
    "        })\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for step in range(training_steps):\n",
    "\n",
    "            # Grab the next batch from the training set\n",
    "            batch_x, batch_length, batch_y = patient_dataset.train.next_batch()\n",
    "      \n",
    "            # Accumulate cost for each epoch\n",
    "            _, step_cost = sess.run([train, cost], feed_dict={\n",
    "                    x: np.swapaxes(batch_x, 0, 1), \n",
    "                    sequence_length: batch_length, \n",
    "                    y: batch_y\n",
    "            })\n",
    "            epoch_loss += step_cost\n",
    "\n",
    "        # Print total loss for each epoch\n",
    "        print 'Epoch', (epoch + 1), 'of', epochs, ': loss of', epoch_loss\n",
    "    \n",
    "    # Output accuracy for validation set\n",
    "    print \"Validation accuracy (after): %2f.\" % sess.run(accuracy, feed_dict={\n",
    "            x: np.swapaxes(valid_x, 0, 1), \n",
    "            sequence_length: valid_length, \n",
    "            y: valid_y \n",
    "        })\n",
    "    \n",
    "    # Grab testing set\n",
    "    test_x = patient_dataset.test.vcg\n",
    "    test_length = patient_dataset.test.vcg_length\n",
    "    test_y = patient_dataset.test.target\n",
    "    \n",
    "    # Output accuracy for testing set\n",
    "    print \"Testing accuracy (After): %2f.\" % sess.run(accuracy, feed_dict={\n",
    "            x: np.swapaxes(test_x, 0, 1), \n",
    "            sequence_length: test_length, \n",
    "            y: test_y \n",
    "        })                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We feed it the validation set before and after to show that the neural network has learned *something*. However, these numbers are far from impressive. We should consider other parameters that could potentially affect the dyssynchrony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3:46AM: \n",
    "\n",
    "Testing accuracy (before): 12.727272.\n",
    "\n",
    "Epoch 0 of 2 loss: 27.0150350332\n",
    "Epoch 1 of 2 loss: 23.9664263725\n",
    "Epoch 2 of 2 loss: 22.5563828945\n",
    "\n",
    "Testing accuracy (After): 57.272728."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "11:27AM:\n",
    "Testing accuracy (before): 23.636364.\n",
    "\n",
    "Epoch 0 of 0 loss: 26.441947937\n",
    "\n",
    "Testing accuracy (After): 44.545452."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
