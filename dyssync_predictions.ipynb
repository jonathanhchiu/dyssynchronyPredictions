{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Dyssynchrony Index\n",
    "   *We assume that the vectorcardiogram and dyssynchrony index have some relation, and hope that machine learning algorithms can accurately find that relationship.* We will be tackling a sequence classification problem with recurrent neural networks. Specifically, we will be classifying vectorcardiogram (VCG) data using a simple LSTM network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dimensions\n",
    "\n",
    "### *Input*\n",
    "Our input will be (simulated) vectorcardiograms generated by Chris Villongco using CMRG's Continuity.\n",
    "The dataset that this example will use consists of 608 VCG's from simulations based on one clinical patient (patient 2, BiV2). We will be truncating each VCG simulation to 120 timesteps. At each time ```t```, we are given the  ```(x, y, z)``` coordinates of the vector head; thus, there are 3 inputs. \n",
    "### *Output*\n",
    "We will be classifying each VCG based on the dyssynchrony index of the simulation that generated it. The dyssynchrony index theoretically ranges from 0 to 1, but we will only be concerned with the range of 0.5 to 1. This range will be further divided into 5 regular intervals. Specifically, these intervals will be ```[0.5, 0.6), [0.6, 0.7), [0.7, 0.8), [0.8, 0.9)```, and ```[0.9, 1.0]```. Simulations with dyssynchrony indices under 0.5 will be considered faulty and will be placed in the first interval, ```[0.5, 0.6)```. Also, note that simulations with dyssynchrony indices of 1.0 will be placed in the last interval, ```[0.9, 1.0)```. These intervals will be labeled as follows (intervals are 0-indexed):\n",
    "* ```0: [0.5, 0.6)```\n",
    "* ```1: [0.6, 0.7)```\n",
    "* ```2: [0.7, 0.8)```\n",
    "* ```3: [0.8, 0.9)```\n",
    "* ```4: [0.9, 1.0]```\n",
    "  \n",
    "Thus, for example, a VCG sequence with a corresponding dyssynchrony index of 0.78 will be placed in class ```\"2\"``` since it falls in the range of 0.7 and 0.8, the third interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Wrapper\n",
    "We've created a class that provides a basic interface for handling the dataset. Specifically, the wrapper will do the following: \n",
    "* Read in the dataset from three specified ```.npy``` files (VCG, VCG lengths, target class)\n",
    "* Split the dataset into training, validation, and testing sets (the set sizes will be fixed for convenience)\n",
    "* Provide a ```next_batch``` function that will return a batch of specified size for a given set.\n",
    "\n",
    "We import the wrapper here. To instantiate, we specify the name of the NumPy files for the VCG and corresponding dyssynchrony indices and designate what percentage of the entire dataset what each set should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vcg import VCG\n",
    "\n",
    "# Initialize dataset iterator\n",
    "data_sets = VCG(\"sequence.npy\", \"target.npy\", 0.6, 0.2, 0.2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: The \"sequence.npy\" VCG data is collapsed, and already randomly permuted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Dimensions\n",
    "We will define the dimensions of our data, as well as the initial hyperparameters of our neural network here. Note: these parameters have not been optimized, they are simply for proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "training_iters = 125\n",
    "batch_size = 20\n",
    "display_step = 10\n",
    "num_hidden = 100\n",
    "\n",
    "# Network Parameters\n",
    "num_steps = 120\n",
    "num_inputs = 3\n",
    "num_classes = 5\n",
    "\n",
    "# Where TensorFlow saves metadata for TensorBoard\n",
    "logs_path='rnndata/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Placeholders\n",
    "We define one placeholder for the VCG input, and another for the index of what class the VCG sequence should be in. This is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# VCG input \n",
    "x = tf.placeholder(\"float\", [None, num_inputs, num_steps])\n",
    "\n",
    "# Index of class the VCG should be categorized as\n",
    "y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation\n",
    "The recurrent neural network creates an output at every timestep. Since this is a problem of sequence classification, we are only interested in the output produced at the last timestep, t=120. We then apply a linear activation on it. The weights and biases are initialized with random values from a normal distribution, with a mean of 0.0 and a standard deviation of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define weights and biases\n",
    "weights = tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "biases = tf.Variable(tf.random_normal([num_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Cell\n",
    "Here we define what kind of recurrent neural network we will be using. We will be using a basic LSTM network with a default forget bias of 1.0, and ```tanh``` as the activation function. The ```BasicLSTMCell``` initializer function takes as parameters:\n",
    "* ```num_units```: The number of units in a LSTM cell.\n",
    "* ``` forget_bias```: float, the bias added to the forget gates.\n",
    "* ``` activation```: activation function of the inner states. Default is ```tanh```.\n",
    "* ``` state_is_tuple```: Accepted and returned states are 2-tuples of the c_state and m_state(???). Default is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import rnn_cell\n",
    "\n",
    "# Define a lstm cell with tensorflow\n",
    "cell = rnn_cell.BasicLSTMCell(num_units=num_hidden, forget_bias=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the ```tf.nn.dynamic_rnn``` function to get the output of the recurrent neural network, instead of the ```tf.nn.rnn``` function. Unlike ```tf.nn.rnn```, ```tf.nn.dynamic_rnn``` takes in sequences longer than the specified length because it used a ``tf.While`` loop to dynamically construct the graph when it is executed. Also, it is faster, despite the fact that ```tf.nn.rnn``` prebuilds the graph. The parameters are as follows:\n",
    "* ```cell```: an instance of RNN cell.\n",
    "* ```inputs```: the RNN input, a single Tensor. The dimensions are [batch_size, sequence_length, num_inputs]\n",
    "* ```sequence_length```: (optional) An int32/64 vector of size [batch_size] specifying the length of each sequence.\n",
    "* ```dtype```: (optional) The data type for the initial state and the expected output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_lengths = []\n",
    "\n",
    "output, states = tf.nn.dynamic_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float64,\n",
    "    #sequence_length=sequence_lengths,\n",
    "    inputs=x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO specify sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
