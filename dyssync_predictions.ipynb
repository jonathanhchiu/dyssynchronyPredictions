{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Dyssynchrony Index\n",
    "We will be tackling a sequence classification problem with recurrent neural networks. We believe that the vectorcardiogram is a good predictor of the dyssynchrony index, so we will treat the VCG as a sequence of coordinates and feed it into a LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dimensions\n",
    "\n",
    "### *Input*\n",
    "Our input will be (simulated) vectorcardiograms generated by Chris Villongco using CMRG's Continuity.\n",
    "Our dataset consists of 608 simulated VCGs. These simulations have varying parameters, such as differing stimulus sites and conduction velocities, but are based on the same patient (patient 2, BiV2). We limit our dataset to only 608 examples in the interest of speeding up computational time, as we are more interested in showing proof of concept than obtaining a higher accuracy (of course, we will aim for higher accuracy later).\n",
    "\n",
    "Because we view the VCG as a sequence, each timestep can be viewed as one element in the sequence. For every time ```t```, we are given three inputs, the  ```(x, y, z)``` coordinates of the vector head.\n",
    "\n",
    "### *Output*\n",
    "We will be classifying each VCG based on the corresponding dyssynchrony index from that same simulation. The dyssynchrony index is a scalar value that theoretically ranges from 0 to 1, but we will only be concerned with the range of 0.5 to 1. This range will be further divided into 5 regular intervals, which will serve as our \"classes\". Specifically, these intervals will be ```[0.5, 0.6), [0.6, 0.7), [0.7, 0.8), [0.8, 0.9)```, and ```[0.9, 1.0]```. Simulations with dyssynchrony indices under 0.5 will be considered faulty and will be placed in the first interval, ```[0.5, 0.6)```. Also, note that simulations with dyssynchrony indices of 1.0 will be placed in the last interval, ```[0.9, 1.0]```. These intervals will be labeled as follows (intervals are 0-indexed):\n",
    "* ```0: [0.5, 0.6)```\n",
    "* ```1: [0.6, 0.7)```\n",
    "* ```2: [0.7, 0.8)```\n",
    "* ```3: [0.8, 0.9)```\n",
    "* ```4: [0.9, 1.0]```\n",
    "  \n",
    "For example, a VCG sequence with a corresponding dyssynchrony index of 0.78 will be placed in class ```\"2\"``` since it falls in the range of 0.7 and 0.8, the third interval.\n",
    "\n",
    "Thus, the output of the neural network will be a probability distribution signifying how likely a simulation with the given VCG will have a dyssynchrony index that falls under each of the five intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Wrapper\n",
    "We've created a class that provides a basic interface for handling the dataset. Specifically, the wrapper will do the following: \n",
    "* Read in the dataset from three specified ```.npy``` files (VCG, VCG lengths, target class)\n",
    "* Split the dataset into training, validation, and testing sets (the set sizes will be fixed for convenience)\n",
    "* Provide a ```next_batch``` function that will return a batch of specified size for a given set.\n",
    "\n",
    "We import the wrapper here. To instantiate, we specify the names of the NumPy files for the following:\n",
    "* ```vcg.npy```: VCG sequences (input)\n",
    "* ```vcg_length.npy```: VCG sequence lengths (passed as argument for ```sequence_length.npy``` parameter)\n",
    "* ```target.npy```: dyssynchrony indices (target output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import Patient\n",
    "\n",
    "# Initialize dataset iterator\n",
    "patient_dataset = Patient(\"dataset/vcg.npy\", \"dataset/vcg_length.npy\", \"dataset/target.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "### Network Dimensions\n",
    "We will define the dimensions of our data, as well as the initial hyperparameters of our neural network here. Note: these parameters have not been optimized, they are simply for proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "training_iters = 125\n",
    "num_hidden = 100\n",
    "\n",
    "# Network Parameters\n",
    "num_steps = 170\n",
    "num_inputs = 3\n",
    "num_classes = 5\n",
    "\n",
    "# Where TensorFlow saves metadata for TensorBoard\n",
    "logs_path='Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Placeholders\n",
    "We define three placeholders. They are for the following:\n",
    "* VCG sequence input: ```[num_steps, batch_size, num_inputs]```\n",
    "* VCG sequence length: ```[batch_size]```\n",
    "* targets (not hot vectors): ```[batch_size]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# VCG input \n",
    "x = tf.placeholder(tf.float32, [num_steps, None, num_inputs])\n",
    "\n",
    "# VCG sequence lengths\n",
    "sequence_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Index of class the VCG should be categorized as\n",
    "y = tf.placeholder(tf.int64, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases\n",
    "The recurrent neural network creates an output at every timestep. Since this is a problem of sequence classification, we are only interested in the output produced at the last timestep, ```t=t_end```. We then apply a linear activation on it. The weights and biases are initialized with random values from a normal distribution, with a mean of 0.0 and a standard deviation of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define weights and biases\n",
    "weights = tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "biases = tf.Variable(tf.random_normal([num_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Cell\n",
    "Here we define what kind of recurrent neural network we will be using. We will be using a basic LSTM network with a default forget bias of 1.0, and ```tanh``` as the activation function. The ```BasicLSTMCell``` initializer function takes as parameters:\n",
    "* ```num_units```: The number of units in a LSTM cell.\n",
    "* ``` forget_bias```: float, the bias added to the forget gates.\n",
    "* ``` activation```: activation function of the inner states. Default is ```tanh```.\n",
    "* ``` state_is_tuple```: Accepted and returned states are 2-tuples of the c_state and m_state(???). Default is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import rnn_cell\n",
    "\n",
    "# Define a lstm cell with tensorflow\n",
    "cell = rnn_cell.BasicLSTMCell(\n",
    "    num_units=num_hidden, \n",
    "    forget_bias=1.0, \n",
    "    state_is_tuple=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Operation\n",
    "We will be using the ```tf.nn.dynamic_rnn``` function, instead of the ```tf.nn.rnn``` function, to get the output of the recurrent neural network. Unlike ```tf.nn.rnn```, ```tf.nn.dynamic_rnn``` takes in variable sequence lengths (it uses a ``tf.While`` loop to dynamically construct the computational graph). Also, it is faster (supposedly), despite the fact that ```tf.nn.rnn``` prebuilds the graph. The parameters are as follows:\n",
    "* ```cell```: an instance of RNN cell.\n",
    "* ```dtype```: (optional) The data type for the initial state and the expected output. \n",
    "* ```sequence_length```: (optional) An int32/64 vector of size [batch_size] specifying the length of each sequence.\n",
    "* ```inputs```: the RNN input, a single Tensor. The dimensions are [batch_size, sequence_length, num_inputs]\n",
    "* ```time_major```: Specifies that the max number of timesteps comes as the first dimensions, so the input placeholder must be of shape ```[max_time, batch_size, num_inputs]```. This requires us to permute the input matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output, states = tf.nn.dynamic_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=sequence_length,\n",
    "    inputs=x,\n",
    "    time_major=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We bring to attention the parameter ```time_major```, which we set to ```True```. This specifies that for our input placeholder, the ```max_time``` will be the *first* dimension, so it must have shape ```[max_time, batch_size, num_inputs]```. As a result, the output tensor would have shape ```[max_time, batch_size, num_hidden]```. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: (170, ?, 100)\n"
     ]
    }
   ],
   "source": [
    "# Shape of output tensor\n",
    "print \"Output tensor shape: \" + str(output.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is advantageous for two reasons:\n",
    "* We can easily access the *last* timestep by calling ```output[-1]``` (we are only concerned with the last timestep as this is a sequence classification problem).\n",
    "* Increases efficiency because it avoids transpositions at the beginning and end of the RNN calculation (https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/recurrent_neural_networks#dynamic_rnn)\n",
    "\n",
    "However, we must alter our input, because they come in shape ```[batch_size, max_time, num_input]```, as it was the more *intuitive* way of bundling our data. Specifically, we must permute the ```0th``` and ```1st``` dimension so that the shape will be ```[max_time, batch_size, num_inputs]```. We can do so by calling NumPy's ```np.swapaxes``` function on ```batch_x``` before passing it into the ```feed_dict```.\n",
    "\n",
    "Below is an example of permuting the ```0th``` and ```1st``` axes of the batch of VCGs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape before swap: (32, 170, 3)\n",
      "Input batch shape after swap: (170, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize a dummy iterator for the purpose of this example\n",
    "dummy_dataset = Patient(\"dataset/vcg.npy\", \"dataset/vcg_length.npy\", \"dataset/target.npy\")\n",
    "\n",
    "# Grab the first batch\n",
    "batch_x, batch_length, batch_target = dummy_dataset.train.next_batch()\n",
    "\n",
    "# Get the shape before permutation\n",
    "print \"Input batch shape before swap: \" + str(batch_x.shape)\n",
    "\n",
    "# Permute the 0th and 1st axes\n",
    "batch_x = np.swapaxes(batch_x, 0, 1)\n",
    "print \"Input batch shape after swap: \" + str(batch_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation\n",
    "After we get the outputs for every timestep, we extract the output for the *last* timestep and apply a linear activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output after linear activation: (?, 5)\n"
     ]
    }
   ],
   "source": [
    "logits = tf.matmul(output[-1], weights) + biases\n",
    "\n",
    "# Shape should be [batch_size, num_classes]\n",
    "print \"Shape of output after linear activation: \" + str(logits.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "Up to this point, we are able to feed forward our input and have the neural network output its \"prediction\". Now we need to set up some key functions to allow for this network to be trained.\n",
    "\n",
    "### Cross Entropy Loss\n",
    "We will be using TensorFlow's ```tf.nn.sparse_softmax_cross_entropy_with_logits``` function which measures the probability error in discrete classification tasks in which the classes are *mutually exclusive*. Note that this operation expects unscaled logits and it performs softmax internally for efficiency. \n",
    "\n",
    "Its parameters are as follows:\n",
    "* logits: float32/64 with shape ```[batch_size, num_classes]```\n",
    "* labels: int32/64 with shape ```[batch_size]``` where each entry is a value between ```[0, num_classes)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of costs (before averaging): (?,)\n"
     ]
    }
   ],
   "source": [
    "# Calculate costs for each example\n",
    "costs = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y)\n",
    "print \"Shape of costs (before averaging): \" + str(costs.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates the cost for each example in the batch. To find the batch cost, we average over all costs in the batch using ```tf.reduce_mean```.\n",
    "\n",
    "Its parameters are as follows:\n",
    "* input_tensor: The tensor to reduce. Should have numeric type.\n",
    "* axis: The dimensions to reduce. If None (the default), reduces all dimensions.\n",
    "* keep_dims: If true, retains reduced dimensions with length 1.\n",
    "* name: A name for the operation (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cost (scalar value): ()\n"
     ]
    }
   ],
   "source": [
    "# Cost function\n",
    "cost = tf.reduce_mean(costs)\n",
    "\n",
    "# Output is a scalar value\n",
    "print \"Shape of cost (scalar value): \" + str(cost.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "Once we define our cost function, we now know what we are trying to minimize. We will use a TensorFlow-defined operation that implements the Adam Algorithm, ```tf.train.AdamOptimizer``` with the following parameters:\n",
    "* learning_rate: A Tensor or a floating point value. The learning rate (default is 0.001)\n",
    "* beta1: A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates (default is 0.9).\n",
    "* beta2: A float value or a constant float tensor. The exponential decay rate for the 2nd moment estimates (default is 0.999).\n",
    "* epsilon: A small constant for numerical stability (default is 1e-08).\n",
    "* use_locking: If True use locks for update operations (default is False).\n",
    "* name: Optional name for the operations created when applying gradients. Defaults to \"Adam\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize TF optimizer\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "    learning_rate=learning_rate,\n",
    "    name=\"AdamOptimizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we initialize an optimizer, we can simply call the ```optimizer.minimize()``` function to both calculate the gradients and apply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training op\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Accuracy\n",
    "Instead of looking at the raw cost to determine the model's performance, we can simply calculate how accurately the model correctly predicted the right class that the VCG falls in. To extract the *class* that the model predicted, we simply call ```tf.argmax``` which returns the index with the largest across a specified axis of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy shape (scalar value): ()\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions with targets\n",
    "compare = tf.equal(tf.argmax(logits, 1), y)\n",
    "\n",
    "# Cast booleans to ints\n",
    "accuracy = tf.reduce_mean(tf.cast(compare, tf.int64))\n",
    "\n",
    "# Shape should be a single scalar value\n",
    "print \"Accuracy shape (scalar value): \" + str(accuracy.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 3, 4, 3, 2, 0, 2, 1, 4, 2, 4, 3, 3, 3, 1, 3, 1, 3, 3, 2, 3,\n",
       "       3, 2, 2, 0, 2, 3, 1, 2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_dataset.train.next_batch()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an Adam optimizer to calculate the gradient with a given learning rate.\n",
    "\n",
    "We calculate and apply the gradients separately because we also want to \n",
    "\n",
    "find the norm of the gradient to use as a stopping criteria if it falls \n",
    "below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients_and_vars = optimizer.compute_gradients(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the norm for each training example\n",
    "gradient_norms = [tf.nn.l2_loss(g) for g, v in gradients_and_vars]\n",
    "\n",
    "# Sum up the norms \n",
    "gradient_norm = tf.add_n(gradient_norms)\n",
    "\n",
    "training_step = optimizer.apply_gradients(gradients_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Placeholder', defined at:\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-dba77c768a80>\", line 4, in <module>\n    x = tf.placeholder(tf.float32, [num_steps, None, num_inputs])\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1212, in placeholder\n    name=name)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1530, in _placeholder\n    name=name)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-349cbea8f000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatient_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#         _, summary = sess.run(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#                         [training_step],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Placeholder', defined at:\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-dba77c768a80>\", line 4, in <module>\n    x = tf.placeholder(tf.float32, [num_steps, None, num_inputs])\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1212, in placeholder\n    name=name)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1530, in _placeholder\n    name=name)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/TTruong/miniconda2/envs/jupyter/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# Initialize all the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    \n",
    "    # Retrieve the entire validation set\n",
    "    valid_x = patient_dataset.validate.vcg\n",
    "    valid_y = patient_dataset.validate.target\n",
    "    \n",
    "    for example in range(training_iters):\n",
    "        \n",
    "        # Train 1 batch a step\n",
    "        batch_x, batch_length, batch_y = patient_dataset.train.next_batch()\n",
    "\n",
    "        sess.run(training_step, feed_dict{sequence: batch_x,})\n",
    "#         _, summary = sess.run(\n",
    "#                         [training_step],\n",
    "#                         feed_dict={sequence_length: batch_x, target: batch_y})\n",
    "        \n",
    "        # Record loss for each step\n",
    "        \n",
    "        # Check if we have overfitted every 50th iteration\n",
    "        \n",
    "    # Determine accuracy\n",
    "    train_x = patient_dataset.train.vcg\n",
    "    train_y = patient_dataset.train.target\n",
    "    \n",
    "    print \"Training Accuracy: %2f\" % sess.run(\n",
    "                        accuracy,\n",
    "                        feed_dict={sequence: train_x, target: train_y})\n",
    "    \n",
    "    print \"Validation Accuracy: %2f\" % sess.run(\n",
    "                        acurracy,\n",
    "                        feed_dict={sequence: valid_x, target: valid_y})\n",
    "\n",
    "    test_x = patient_dataset.test.vcg\n",
    "    test_y = patient_dataset.test.target\n",
    "    \n",
    "    print \"Testing Accuracy: %2f\" % sess.run(\n",
    "                        accuracy,\n",
    "                        feed_dict={sequence: test_x, target: test_y})\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
