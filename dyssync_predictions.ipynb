{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Dyssynchrony Index\n",
    "We will be classifying vectorcardiogram (VCG) data using a recurrent neural network. The VCG input are simulations generated from CMRG's Continuity software by Chris Villongco. The classes that we are placing these VCGs correspond to the dyssynchrony index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dimensions\n",
    "### *Input*\n",
    "Our current dataset consists of 608 VCG simulations based on 8 clinical patients. We will be truncating each VCG simulation to 120 timesteps. Each timestep contains the x, y, z coordinates of the head of the vector; thus, there are 3 inputs. \n",
    "### *Output*\n",
    "We will be classifying each VCG simulation based on the corresponding dyssynchrony index of that simulation. The dyssynchrony index ranges from 0 to 1, but we will only be concerned with the range of 0.5 to 1. This range will be further divided into 5 regular intervals. For example, a VCG sequence with a dyssynchrony index of 0.78 will be placed in class \"3\" since it falls in the range of 0.6 and 0.7, the third interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Wrapper\n",
    "We've created a class that provides a basic interface for handling the dataset. Specifically, the wrapper will do the following: \n",
    "* Read in the dataset from a .np file\n",
    "* Split the dataset into the training, validation, and testing sets\n",
    "* Provide a ```next_batch``` function that will return a batch of specified size for a certain set.\n",
    "\n",
    "We import the wrapper here. To instantiate, we specify the name of the NumPy files for the VCG and corresponding dyssynchrony indices and designate what percentage of the entire dataset what each set should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vcg import VCG\n",
    "\n",
    "# Initialize dataset iterator\n",
    "data_sets = VCG(\"sequence.npy\", \"target.npy\", 0.6, 0.2, 0.2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: The \"sequence.npy\" VCG data is collapsed, and already randomly permuted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Dimensions\n",
    "We will define the dimensions of our neural network, as well as its initial hyperparameters. Note: these parameters have not been optimized, they are simply for proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "training_iters = 125\n",
    "batch_size = 20\n",
    "display_step = 10\n",
    "num_hidden = 100\n",
    "\n",
    "# Network Parameters\n",
    "num_steps = 120\n",
    "num_inputs = 3\n",
    "num_classes = 5\n",
    "\n",
    "# Where TensorFlow saves metadata for TensorBoard\n",
    "logs_path='rnndata/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Placeholders\n",
    "We define one placeholder for the VCG input, and another for the index of what class the VCG sequence should be in. This is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# VCG input \n",
    "x = tf.placeholder(\"float\", [None, num_inputs, num_steps])\n",
    "\n",
    "# Index of class the VCG should be categorized as\n",
    "y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation\n",
    "The recurrent neural network creates an output at every timestep. Since this is a problem of sequence classification, we are only interested in the output produced at the last timestep, t=120. We then apply a linear activation on it. The weights and biases are initialized with random values from a normal distribution, with a mean of 0.0 and a standard deviation of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define weights and biases\n",
    "weights = tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "biases = tf.Variable(tf.random_normal([num_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Cell\n",
    "Here we define what kind of recurrent neural network we will be using. We will be using a basic LSTM network with a default forget bias of 1.0, and ```tanh``` as the activation function. The ```BasicLSTMCell``` initializer function takes as parameters:\n",
    "* ```num_units```: The number of units in a LSTM cell.\n",
    "* ``` forget_bias```: float, the bias added to the forget gates.\n",
    "* ``` activation```: activation function of the inner states. Default is ```tanh```.\n",
    "* ``` state_is_tuple```: Accepted and returned states are 2-tuples of the c_state and m_state(???). Default is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import rnn_cell\n",
    "\n",
    "# Define a lstm cell with tensorflow\n",
    "cell = rnn_cell.BasicLSTMCell(num_units=num_hidden, forget_bias=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the ```tf.nn.dynamic_rnn``` function to get the output of the recurrent neural network, instead of the ```tf.nn.rnn``` function. Unlike ```tf.nn.rnn```, ```tf.nn.dynamic_rnn``` takes in sequences longer than the specified length because it used a ``tf.While`` loop to dynamically construct the graph when it is executed. Also, it is faster, despite the fact that ```tf.nn.rnn``` prebuilds the graph. The parameters are as follows:\n",
    "* ```cell```: an instance of RNN cell.\n",
    "* ```inputs```: the RNN input, a single Tensor. The dimensions are [batch_size, sequence_length, num_inputs]\n",
    "* ```sequence_length```: (optional) An int32/64 vector of size [batch_size] specifying the length of each sequence.\n",
    "* ```dtype```: (optional) The data type for the initial state and the expected output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_lengths = []\n",
    "\n",
    "output, states = tf.nn.dynamic_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float64,\n",
    "    #sequence_length=sequence_lengths,\n",
    "    inputs=x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO specify sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
