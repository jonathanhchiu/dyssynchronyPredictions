{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Dyssynchrony Index\n",
    "We will be tackling a sequence classification problem with recurrent neural networks. We believe that the vectorcardiogram is a good predictor of the dyssynchrony index, so we will treat the VCG as a sequence of coordinates and feed it into a LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dimensions\n",
    "\n",
    "### *Input*\n",
    "Our input will be (simulated) vectorcardiograms generated by Chris Villongco using CMRG's Continuity.\n",
    "Our dataset consists of 608 simulated VCGs. These simulations have varying parameters, such as differing stimulus sites and conduction velocities, but are based on the same patient (patient 2, BiV2). We limit our dataset to only 608 examples in the interest of speeding up computational time, as we are more interested in showing proof of concept than obtaining a higher accuracy (of course, we will aim for higher accuracy later).\n",
    "\n",
    "Because we view the VCG as a sequence, each timestep can be viewed as one element in the sequence. For every time ```t```, we are given three inputs, the  ```(x, y, z)``` coordinates of the vector head.\n",
    "\n",
    "### *Output*\n",
    "We will be classifying each VCG based on the corresponding dyssynchrony index from that same simulation. The dyssynchrony index theoretically ranges from 0 to 1, but we will only be concerned with the range of 0.5 to 1. This range will be further divided into 5 regular intervals. Specifically, these intervals will be ```[0.5, 0.6), [0.6, 0.7), [0.7, 0.8), [0.8, 0.9)```, and ```[0.9, 1.0]```. Simulations with dyssynchrony indices under 0.5 will be considered faulty and will be placed in the first interval, ```[0.5, 0.6)```. Also, note that simulations with dyssynchrony indices of 1.0 will be placed in the last interval, ```[0.9, 1.0]```. These intervals will be labeled as follows (intervals are 0-indexed):\n",
    "* ```0: [0.5, 0.6)```\n",
    "* ```1: [0.6, 0.7)```\n",
    "* ```2: [0.7, 0.8)```\n",
    "* ```3: [0.8, 0.9)```\n",
    "* ```4: [0.9, 1.0]```\n",
    "  \n",
    "Thus, for example, a VCG sequence with a corresponding dyssynchrony index of 0.78 will be placed in class ```\"2\"``` since it falls in the range of 0.7 and 0.8, the third interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Wrapper\n",
    "We've created a class that provides a basic interface for handling the dataset. Specifically, the wrapper will do the following: \n",
    "* Read in the dataset from three specified ```.npy``` files (VCG, VCG lengths, target class)\n",
    "* Split the dataset into training, validation, and testing sets (the set sizes will be fixed for convenience)\n",
    "* Provide a ```next_batch``` function that will return a batch of specified size for a given set.\n",
    "\n",
    "We import the wrapper here. To instantiate, we specify the names of the NumPy files for the following:\n",
    "* ```vcg.npy```: VCG sequences (input)\n",
    "* ```vcg_length.npy```: VCG sequence lengths (passed as argument for ```sequence_length.npy``` parameter)\n",
    "* ```target.npy```: dyssynchrony indices (target output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import Patient\n",
    "\n",
    "# Initialize dataset iterator\n",
    "patient_dataset = Patient(\"vcg.npy\", \"vcg_length.npy\", \"target.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Dimensions\n",
    "We will define the dimensions of our data, as well as the initial hyperparameters of our neural network here. Note: these parameters have not been optimized, they are simply for proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "training_iters = 125\n",
    "batch_size = 20\n",
    "display_step = 10\n",
    "num_hidden = 100\n",
    "\n",
    "# Network Parameters\n",
    "num_steps = 120\n",
    "num_inputs = 3\n",
    "num_classes = 5\n",
    "\n",
    "# Where TensorFlow saves metadata for TensorBoard\n",
    "logs_path='Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Placeholders\n",
    "We define three placeholders. They are for the following:\n",
    "* VCG sequence input\n",
    "* VCG sequence length\n",
    "* targets (used for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# VCG input \n",
    "inputs = tf.placeholder(tf.float32, [None, num_inputs, num_steps])\n",
    "\n",
    "# VCG sequence lengths\n",
    "sequence_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Index of class the VCG should be categorized as\n",
    "y = tf.placeholder(tf.float32, [None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "The recurrent neural network creates an output at every timestep. Since this is a problem of sequence classification, we are only interested in the output produced at the last timestep, ```t=t_end```. We then apply a linear activation on it. The weights and biases are initialized with random values from a normal distribution, with a mean of 0.0 and a standard deviation of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define weights and biases\n",
    "weights = tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "biases = tf.Variable(tf.random_normal([num_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Cell\n",
    "Here we define what kind of recurrent neural network we will be using. We will be using a basic LSTM network with a default forget bias of 1.0, and ```tanh``` as the activation function. The ```BasicLSTMCell``` initializer function takes as parameters:\n",
    "* ```num_units```: The number of units in a LSTM cell.\n",
    "* ``` forget_bias```: float, the bias added to the forget gates.\n",
    "* ``` activation```: activation function of the inner states. Default is ```tanh```.\n",
    "* ``` state_is_tuple```: Accepted and returned states are 2-tuples of the c_state and m_state(???). Default is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import rnn_cell\n",
    "\n",
    "# Define a lstm cell with tensorflow\n",
    "cell = rnn_cell.BasicLSTMCell(num_units=num_hidden, forget_bias=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the ```tf.nn.dynamic_rnn``` function, instead of the ```tf.nn.rnn``` function, to get the output of the recurrent neural network. Unlike ```tf.nn.rnn```, ```tf.nn.dynamic_rnn``` takes in variable sequence lengths (it uses a ``tf.While`` loop to dynamically construct the computational graph). Also, it is faster (supposedly), despite the fact that ```tf.nn.rnn``` prebuilds the graph. The parameters are as follows:\n",
    "* ```cell```: an instance of RNN cell.\n",
    "* ```inputs```: the RNN input, a single Tensor. The dimensions are [batch_size, sequence_length, num_inputs]\n",
    "* ```sequence_length```: (optional) An int32/64 vector of size [batch_size] specifying the length of each sequence.\n",
    "* ```dtype```: (optional) The data type for the initial state and the expected output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "output, states = tf.nn.dynamic_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=sequence_length,\n",
    "    inputs=inputs\n",
    ")\n",
    "\n",
    "print \"hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO specify sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
