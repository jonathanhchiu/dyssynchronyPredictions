{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Wrapper\n",
    "I will be explaining the reasoning behind the implementation of the dataset wrapper for the vectorcardiograms and dyssynchrony indices.\n",
    "\n",
    "## Goal\n",
    "The goal is to be able to iterate through a given dataset with minimum effort for the purpose of training a neural network in batches. Ideally, we would want to be able to call ```next_batch``` and it would give us the next batch of a specified size within the dataset. \n",
    "\n",
    "## Next Batch\n",
    "We provide additional requirements of the ```next_batch``` function here. They are as follows:\n",
    "* Deliver a specified number of examples upon calling ```next_batch``` for both the dyssynchrony index and the vectorcardiogram\n",
    "* We deliver the batches sequentially. For example, if we deliver the the first batch, it should be example numbers 1 through 10. The second batch should deliver example numbers 11-20. There are exceptions for corner cases however (such as when the specified batch size is greater than the dataset size, or if we have reached the end of the dataset and need to pull from the beginning).\n",
    "* If we have iterated through the entire dataset, then start pulling batches from the beginning. This is reasonable because most often, neural networks are usually trained with more than one epoch (the network sees the entire dataset usually more than once)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Steps\n",
    "### *Step 1: Rename Files*\n",
    "We wish to rename the files for three reasons:\n",
    "* Impose an ordering on the example numbers \n",
    "* Make the filenames more readable\n",
    "* Maintain that the filenames are predictable and follow a well-defined format\n",
    "\n",
    "Thus, we will execute the following bash script to rename all the ```.txt``` files in the current directory:\n",
    "```\n",
    "a=1\n",
    "for file in allParams-1_ECG_VCG_{1..608}_dump.txt; do \n",
    "    \n",
    "    # Require a 3 digit padding\n",
    "    new=$(printf \"version%03d.txt\" \"$a\")\n",
    "    \n",
    "    # Change the name\n",
    "    mv \"$file\" \"$new\"\n",
    "    let a=a+1\n",
    "done\n",
    "```\n",
    "Since the original filenames were not zero-padded, when we ```ls```, we would get ```allParams-1_ECG_VCG_109_dump.txt``` lexicographically before ```allParams-1_ECG_VCG_10_dump.txt```. Thus, instead of iterating through each ```file in ls *.txt```, we have to iterate through them using curly braces ```{1..608}``` to maintain that ```allParams-1_ECG_VCG_9_dump.txt``` corresponds to ```version009.txt``` and not ```allParams-1_ECG_VCG_109_dump.txt```\n",
    "\n",
    "To show that this has preserved the original ordering, look at the content of the first file before and after the renaming.\n",
    "\n",
    "#### Before:\n",
    "```\n",
    ">>> head allParams-1_ECG_VCG_1_dump.txt\n",
    "1.14859698e-06\t-8.52689793e-07\t-1.62738886e-07\n",
    "6.27637865e-03\t8.56158099e-04\t-2.80092680e-05\n",
    "1.73977577e-02\t2.37706707e-03\t-8.70847085e-05\n",
    "0.03220872\t0.00439809\t-0.00017971\n",
    "0.04663505\t0.00636597\t-0.00028655\n",
    "0.05990819\t0.00821321\t-0.00043461\n",
    "0.07573148\t0.01035879\t-0.00061512\n",
    "0.09897242\t0.01347624\t-0.0008311\n",
    "0.11859204\t0.01606282\t-0.00100526\n",
    "0.13539736\t0.01838106\t-0.00139427\n",
    "```\n",
    "\n",
    "#### After:\n",
    "```\n",
    ">>> head version001.txt\n",
    "1.14859698e-06\t-8.52689793e-07\t-1.62738886e-07\n",
    "6.27637865e-03\t8.56158099e-04\t-2.80092680e-05\n",
    "1.73977577e-02\t2.37706707e-03\t-8.70847085e-05\n",
    "0.03220872\t0.00439809\t-0.00017971\n",
    "0.04663505\t0.00636597\t-0.00028655\n",
    "0.05990819\t0.00821321\t-0.00043461\n",
    "0.07573148\t0.01035879\t-0.00061512\n",
    "0.09897242\t0.01347624\t-0.0008311\n",
    "0.11859204\t0.01606282\t-0.00100526\n",
    "0.13539736\t0.01838106\t-0.00139427\n",
    "```\n",
    "It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Step 2: Convert To NumPy Arrays*\n",
    "We wish to read in each VCG data as a NumPy 2D matrix and store them all as a 3D matrix. Similarly, we wish to read in each dyssynchrony index, a scalar value, and store them all as a column vector. We will read them in with a simple python script, provided below (We provided the script as Markdown because it only needs to be executed once, and we have done it for you):\n",
    "\n",
    "#### Read in Vectorcardiogram Simulations with Python\n",
    "```\n",
    "# read_vcg.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize python list containing the vcg lengths and input \n",
    "vcg_length = []\n",
    "vcg = []\n",
    "\n",
    "for index in range(608):\n",
    "\n",
    "\t# Create filename with zero pad\n",
    "\tfilename = 'version{:03d}.txt'.format(index + 1)\n",
    "\n",
    "\t# Read in the text file as numpy matrix\n",
    "\tx = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "\tvcg.append(x)\n",
    "\n",
    "\t# Grab and store the vcg length\n",
    "\tvcg_length.append(x.shape[0])\n",
    "\n",
    "# Convert Python list to NumPy array and save\n",
    "np_vcg_length = np.asarray(vcg_length, dtype=np.int32)\n",
    "np.save(\"vcg_length.npy\", np_vcg_length)\n",
    "\n",
    "np_vcg = np.asarray(vcg, dtype=np.float32)\n",
    "np.save(\"vcg.npy\", np_vcg)\n",
    "\n",
    "```\n",
    "\n",
    "The result should be two files: ```vcg.npy``` and ```vcg_length.npy```, saved in our current directory. We save the length of each VCG simulation because the TensorFlow function ```tf.nn.dynamic_rnn``` accepts the optional parameter ```sequence_length```, an int32/int64 vector sized [batch_size] as a way of checking that the dimensions of our data reflect our design.\n",
    "\n",
    "#### Read in Dyssynchrony Indices With Python\n",
    "We do a similar thing with the corresponding dyssynchrony indices, but they are easier since they all lie in a single file. We run the following script (We provided the script as Markdown because it only needs to be executed once, and we have done it for you):\n",
    "```\n",
    "# read_dyssync.py\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "dyssync = np.loadtxt(\"dyssync.txt\")\n",
    "np.save(\"dyssync.npy\", dyssync)\n",
    "```\n",
    "\n",
    "#### Converting the Dyssynchrony Indices to Class Indices\n",
    "When we save the first file, ```dyssync.npy```, each entry is a real value between ```0.5``` and ```1```. However, we need to map it to the set of indices of classes, namely, the integers between ```1``` and ```5```. The mapping is as follows:\n",
    "* Multiply by 10.\n",
    "* Floor the result.\n",
    "* Subtract 5.\n",
    "* Corner case: if the dyssynchrony index is less than 0.5 (0 is common), then we set those entries to ```0``` (instead of ```0*10 - 5 = -5```).\n",
    "* Corner case: if the dyssynchrony index is exactly 1, then we set those entries to ```4``` (instead of ```1*10 - 5 = 5```).\n",
    "* Convert to int datatype.\n",
    "\n",
    "We execute the following python script to implement the mapping and save it as a new ```.npy``` file:\n",
    "```\n",
    "# mapping.py\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "# Load the column vector containing the dyssynchrony indices\n",
    "init_x = np.load(\"dyssync.npy\")\n",
    "\n",
    "# Multiply elementwise by 10, floor result, subtract 5\n",
    "x_scaled = np.multiply(init_x, 10)\n",
    "x_floor = np.floor(x_scaled)\n",
    "x = np.subtract(x_floor, 5)\n",
    "\n",
    "# Corner case: dyssynchrony index is between [0, 0.5)\n",
    "x[x < 0] = 0\n",
    "\n",
    "# Corner case: dyssynchrony index is 1.0\n",
    "x[x > 4] = 4\n",
    "\n",
    "# Convert each element to int \n",
    "x = x.astype(int)\n",
    "\n",
    "# Save to file \n",
    "np.save(\"target.npy\", x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Playing with the NumPy Dataset\n",
    "We will play around with the NumPy matrices that we have just created. The files of interest are as follows:\n",
    "* ```vcg.npy``` The VCG itself\n",
    "* ```vcg_length.npy``` The length of each VCG sequence\n",
    "* ```target.npy``` The class indices that the corresponding VCG lands in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCG Dimensions: (608, 170, 3)\n",
      "VCG Sequence Length Dimensions: (608,)\n",
      "VCG Sequence Class Indices: (608,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import our length and vcg sequence and class index\n",
    "vcg = np.load(\"vcg.npy\")\n",
    "vcg_length = np.load(\"vcg_length.npy\")\n",
    "target = np.load(\"target.npy\")\n",
    "\n",
    "print \"VCG Dimensions: \" + str(vcg.shape)\n",
    "print \"VCG Sequence Length Dimensions: \" + str(vcg_length.shape)\n",
    "print \"VCG Sequence Class Indices: \" + str(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.14859699e-06,  -8.52689766e-07,  -1.62738885e-07],\n",
       "       [  6.27637887e-03,   8.56158091e-04,  -2.80092681e-05],\n",
       "       [  1.73977576e-02,   2.37706699e-03,  -8.70847070e-05],\n",
       "       [  3.22087184e-02,   4.39808983e-03,  -1.79709998e-04],\n",
       "       [  4.66350503e-02,   6.36596978e-03,  -2.86549999e-04]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five timesteps of the 0th simulation\n",
    "vcg[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([170, 170, 170, 170, 170], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five VCG sequence lengths\n",
    "vcg_length[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data type of the ```vcg.npy``` matrix is a 32 bit float whereas the data type of the ```vcg_length.npy``` matrix is a 32 bit int, to match the parameter requirements of ```sequence_length``` in ```tf.nn.dynamic_rnn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCG Sequences #1-5 falls in classes: [2 2 2 2 3]\n",
      "VCG Sequence #187 falls in class: 0\n"
     ]
    }
   ],
   "source": [
    "# Print the first five class indices\n",
    "print \"VCG Sequences #1-5 falls in classes: \" + str(target[:5])\n",
    "\n",
    "# VCG Sequence number 187 has a dyssynchrony index of 0.0\n",
    "# It should be mapped to class 0 instead of -5\n",
    "print \"VCG Sequence #187 falls in class: \" + str(target[186])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the necessary data in the correct format; we are now ready to create our wrapper class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Step 3: Create a Wrapper Class*\n",
    "Now that we've saved the dataset as NumPy files, we now create a Python class that can:\n",
    "* Provide the ```next_batch``` function.\n",
    "* Randomize the dataset, and store how we randomized it.\n",
    "* Store the sequence lengths of each VCG.\n",
    "* Allow the user to specify how to split the data set into training, validation, and testing.\n",
    "\n",
    "This class will be called ```Simulations```. Within it contains three members, ```train, validate,``` and ```test```, which correspond to the three subsets we will divide our dataset into. For clarification, the purpose of the three sets are as follows:\n",
    "* Training set: a set of examples used for learning the weights of the classifier.\n",
    "* Validation set: a set of examples used to tune the hyperparameters (architectures, not weights) of a classifier. We can use this to determine the optimum number of hidden units in a neural network.\n",
    "* Testing set: a set of examples used ONLY to access the performance of a fully specified classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
