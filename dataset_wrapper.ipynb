{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Wrapper\n",
    "I will be explaining the reasoning behind the implementation of the dataset wrapper for the vectorcardiograms and dyssynchrony indices.\n",
    "\n",
    "## Goal\n",
    "The goal is to be able to easily iterate through a given dataset when we are training a neural network. Ideally, we would want to be able to call ```next_batch``` and it would give us the next batch of a specified size within the dataset. \n",
    "\n",
    "## Next Batch\n",
    "We provide additional requirements of the ```next_batch``` function here. They are as follows:\n",
    "* Deliver a specified number of examples upon calling ```next_batch``` for the vectorcardiograms sequences, vectorcardiogram lengths, and the dyssynchrony indices.\n",
    "* The batches are delivered sequentially. For example, if we deliver the the first batch, it should contain example numbers 0 through 9. The second batch should deliver example numbers 10 through 20.\n",
    "* If we have iterated through the entire dataset, then start pulling batches from the beginning. This is reasonable because most often, neural networks are usually trained with more than one epoch (the network sees the entire dataset usually more than once)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Steps\n",
    "### *Step 0: Setup*\n",
    "To download the dataset, please email Chris Villongco for the link to the Dropbox. Once you have access, download the folder ```BiV2``` that corresponds to Patient 2 (our choice of Patient 2 is arbitrary, and we can use any of the patients available). Within that folder, we find the following files and folders:\n",
    "* ```vcg_measured```* The actual (recorded) VCG (not simulated)\n",
    "* ```vcg_model/``` The VCG simulations\n",
    "* ```BiV2_LVdyssync_opt.txt```* The calculated dyssynchrony index (not from simulations)\n",
    "* ```BiV2_LVdyssync.txt``` The dyssynchrony indices resulting from the simulations\n",
    "* ```pts_to_eval.txt```* The parameters of the simulations\n",
    "\n",
    "\\* We are not interested in these files.\n",
    "\n",
    "Once you download and extract it (usually comes as a .zip file), run the following:\n",
    "```\n",
    ">>> cd BiV2/vcg_model\n",
    "```\n",
    "and we are ready to begin!\n",
    "\n",
    "### *Step 1: Rename Files*\n",
    "We wish to rename the files for three reasons:\n",
    "* Impose an ordering on the example numbers that is clearly visible in the file name\n",
    "* Make the filenames more readable\n",
    "* *Maintain* that the filenames are predictable and follow a well-defined format\n",
    "\n",
    "Thus, we will execute the following bash script to rename all the ```.txt``` files in the current directory into something more readable:\n",
    "```\n",
    "a=1\n",
    "for file in allParams-1_ECG_VCG_{1..608}_dump.txt; do \n",
    "    \n",
    "    # Require a 3 digit padding\n",
    "    new=$(printf \"version%03d.txt\" \"$a\")\n",
    "    \n",
    "    # Change the name\n",
    "    mv \"$file\" \"$new\"\n",
    "    let a=a+1\n",
    "done\n",
    "```\n",
    "The result will be 608 files that are renamed to ```version%%%.txt```, where the number is padded with three digits.\n",
    "\n",
    "\n",
    "Since the original filenames were not zero-padded, when we ```ls```, we would get ```allParams-1_ECG_VCG_109_dump.txt``` lexicographically before ```allParams-1_ECG_VCG_10_dump.txt```. Thus, instead of iterating through each ```file in ls *.txt```, we have to iterate through them using curly braces ```{1..608}``` to maintain that ```allParams-1_ECG_VCG_9_dump.txt``` corresponds to ```version009.txt``` and not ```allParams-1_ECG_VCG_109_dump.txt```\n",
    "\n",
    "To show that this has preserved the original ordering, look at the content of the file labelled ```1``` before and after the renaming.\n",
    "\n",
    "#### Before:\n",
    "```\n",
    ">>> head allParams-1_ECG_VCG_1_dump.txt\n",
    "1.14859698e-06\t-8.52689793e-07\t-1.62738886e-07\n",
    "6.27637865e-03\t8.56158099e-04\t-2.80092680e-05\n",
    "1.73977577e-02\t2.37706707e-03\t-8.70847085e-05\n",
    "0.03220872\t0.00439809\t-0.00017971\n",
    "0.04663505\t0.00636597\t-0.00028655\n",
    "0.05990819\t0.00821321\t-0.00043461\n",
    "0.07573148\t0.01035879\t-0.00061512\n",
    "0.09897242\t0.01347624\t-0.0008311\n",
    "0.11859204\t0.01606282\t-0.00100526\n",
    "0.13539736\t0.01838106\t-0.00139427\n",
    "```\n",
    "\n",
    "#### After:\n",
    "```\n",
    ">>> head version001.txt\n",
    "1.14859698e-06\t-8.52689793e-07\t-1.62738886e-07\n",
    "6.27637865e-03\t8.56158099e-04\t-2.80092680e-05\n",
    "1.73977577e-02\t2.37706707e-03\t-8.70847085e-05\n",
    "0.03220872\t0.00439809\t-0.00017971\n",
    "0.04663505\t0.00636597\t-0.00028655\n",
    "0.05990819\t0.00821321\t-0.00043461\n",
    "0.07573148\t0.01035879\t-0.00061512\n",
    "0.09897242\t0.01347624\t-0.0008311\n",
    "0.11859204\t0.01606282\t-0.00100526\n",
    "0.13539736\t0.01838106\t-0.00139427\n",
    "```\n",
    "The first ten lines appear to match. It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Step 2: Convert To NumPy Arrays*\n",
    "We wish to read in each VCG data as a NumPy 2D matrix and store them all in a list (creating a list of 2D matrices, thus becoming 3D). Similarly, we wish to read in each dyssynchrony index, a scalar value, and store them all as a column vector. We will read them in with simple python scripts, provided below (We provided the script as Markdown because it only needs to be executed once, and we have done it for you):\n",
    "\n",
    "#### Read in Vectorcardiogram Simulations with Python\n",
    "```\n",
    "# read_vcg.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize python list containing the vcg lengths and input \n",
    "vcg_length = []\n",
    "vcg = []\n",
    "\n",
    "for index in range(608):\n",
    "\n",
    "\t# Create filename with zero pad\n",
    "\tfilename = 'version{:03d}.txt'.format(index + 1)\n",
    "\n",
    "\t# Read in the text file as numpy matrix\n",
    "\tx = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "\tvcg.append(x)\n",
    "\n",
    "\t# Grab and store the vcg length\n",
    "\tvcg_length.append(x.shape[0])\n",
    "\n",
    "# Convert Python list to NumPy array and save\n",
    "np_vcg_length = np.asarray(vcg_length, dtype=np.int32)\n",
    "np.save(\"vcg_length.npy\", np_vcg_length)\n",
    "\n",
    "np_vcg = np.asarray(vcg, dtype=np.float32)\n",
    "np.save(\"vcg.npy\", np_vcg)\n",
    "\n",
    "```\n",
    "\n",
    "The result should be two files: ```vcg.npy``` and ```vcg_length.npy```, saved in our current directory. We save the length of each VCG simulation because the TensorFlow function ```tf.nn.dynamic_rnn``` accepts the optional parameter ```sequence_length```, an int32/int64 vector sized [batch_size] as a way of checking that the dimensions of our data reflect our design.\n",
    "\n",
    "#### Read in Dyssynchrony Indices With Python\n",
    "We do a similar thing with the corresponding dyssynchrony indices, but they are easier since they all lie in a single file. We run the following script (We provided the script as Markdown because it only needs to be executed once, and we have done it for you):\n",
    "```\n",
    "# read_dyssync.py\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "dyssync = np.loadtxt(\"dyssync.txt\")\n",
    "np.save(\"dyssync.npy\", dyssync)\n",
    "```\n",
    "\n",
    "#### Converting the Dyssynchrony Indices to Class Indices\n",
    "When we save the first file, ```dyssync.npy```, each entry is a real number value between ```0.5``` and ```1```. However, we need to map it to the set of class indices (what each VCG belongs to), namely, the integers between ```0``` and ```4```. The mapping is as follows:\n",
    "* Multiply by 10.\n",
    "* Floor the result.\n",
    "* Subtract 5.\n",
    "* Corner case: if the dyssynchrony index is less than 0.5 (0 is common), then we set those entries to ```0``` (instead of ```0*10 - 5 = -5```).\n",
    "* Corner case: if the dyssynchrony index is exactly 1, then we set those entries to ```4``` (instead of ```1*10 - 5 = 5```).\n",
    "* Convert to int datatype.\n",
    "\n",
    "We execute the following python script to implement the mapping and save it as a new ```.npy``` file:\n",
    "```\n",
    "# mapping.py\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "# Load the column vector containing the dyssynchrony indices\n",
    "init_x = np.load(\"dyssync.npy\")\n",
    "\n",
    "# Multiply elementwise by 10, floor result, subtract 5\n",
    "x_scaled = np.multiply(init_x, 10)\n",
    "x_floor = np.floor(x_scaled)\n",
    "x = np.subtract(x_floor, 5)\n",
    "\n",
    "# Corner case: dyssynchrony index is between [0, 0.5)\n",
    "x[x < 0] = 0\n",
    "\n",
    "# Corner case: dyssynchrony index is 1.0\n",
    "x[x > 4] = 4\n",
    "\n",
    "# Convert each element to int \n",
    "x = x.astype(int)\n",
    "\n",
    "# Save to file \n",
    "np.save(\"target.npy\", x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Playing with the NumPy Dataset\n",
    "We will play around with the NumPy matrices that we have just created. The files of interest are as follows:\n",
    "* ```vcg.npy``` The VCG itself\n",
    "* ```vcg_length.npy``` The length of each VCG sequence\n",
    "* ```target.npy``` The class indices that the corresponding VCG lands in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCG Dimensions: (608, 170, 3)\n",
      "VCG Sequence Length Dimensions: (608,)\n",
      "VCG Sequence Class Indices: (608,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import our length and vcg sequence and class index\n",
    "vcg = np.load(\"dataset/vcg.npy\")\n",
    "vcg_length = np.load(\"dataset/vcg_length.npy\")\n",
    "target = np.load(\"dataset/target.npy\")\n",
    "\n",
    "print \"VCG Dimensions: \" + str(vcg.shape)\n",
    "print \"VCG Sequence Length Dimensions: \" + str(vcg_length.shape)\n",
    "print \"VCG Sequence Class Indices: \" + str(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.14859699e-06,  -8.52689766e-07,  -1.62738885e-07],\n",
       "       [  6.27637887e-03,   8.56158091e-04,  -2.80092681e-05],\n",
       "       [  1.73977576e-02,   2.37706699e-03,  -8.70847070e-05],\n",
       "       [  3.22087184e-02,   4.39808983e-03,  -1.79709998e-04],\n",
       "       [  4.66350503e-02,   6.36596978e-03,  -2.86549999e-04]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five timesteps of the 0th simulation\n",
    "vcg[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([170, 170, 170, 170, 170], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five VCG sequence lengths\n",
    "vcg_length[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data type of each entry in the ```vcg.npy``` matrix is a 32 bit float whereas the data type of the ```vcg_length.npy``` matrix is a 32 bit int, to match the parameter requirements of ```sequence_length``` in ```tf.nn.dynamic_rnn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All VCGs have 170 timesteps.\n"
     ]
    }
   ],
   "source": [
    "# Check that the length of each VCG is exactly 170 for Patient 2\n",
    "if vcg_length[vcg_length < 170]:\n",
    "    print \"Exists a VCG with a length not equal to 170 timesteps.\"\n",
    "else:\n",
    "    print \"All VCGs have 170 timesteps.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all VCG's for Patient 2 have exactly 170 timesteps. It is possible that this feature does not exist in other patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCG Sequences #1-5 falls in classes: [2 2 2 2 3]\n",
      "VCG Sequence #187 falls in class: 0\n"
     ]
    }
   ],
   "source": [
    "# Print the first five class indices\n",
    "print \"VCG Sequences #1-5 falls in classes: \" + str(target[:5])\n",
    "\n",
    "# VCG Sequence number 187 has a dyssynchrony index of 0.0\n",
    "# It should be mapped to class 0 instead of -5\n",
    "print \"VCG Sequence #187 falls in class: \" + str(target[186])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the necessary data in the correct format; we are now ready to create our wrapper class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Step 3: Create a Wrapper Class*\n",
    "Now that we've saved the dataset as NumPy files, we now create a Python class that can:\n",
    "* Provide the ```next_batch``` function.\n",
    "* Randomize the dataset, and store how we randomized it.\n",
    "* Store the sequence lengths of each VCG.\n",
    "\n",
    "This class will be called ```Simulations```. Within it, contains three members, ```train, validate,``` and ```test```, which correspond to the three subsets we will divide our dataset into. For clarification, the purpose of the three sets are as follows:\n",
    "* Training set: a set of examples used for learning the weights of the classifier.\n",
    "* Validation set: a set of examples used to tune the hyperparameters (architectures, not weights) of a classifier. We can use this to determine the optimum number of hidden units in a neural network.\n",
    "* Testing set: a set of examples used ONLY to access the performance of a fully specified classifier.\n",
    "\n",
    "For the purpose of simplifying implementation, we will pre-partition the training, validation, and testing sets, as well as fix the batch size to ```32``` examples. We will try to get the split as close to a ```60%, 20%, 20%``` split, but we will keep the set sizes divisble by ```32```. Thus, our set sizes will be as follows:\n",
    "* Training set: 416 examples, 13 batches (~68%)\n",
    "* Validation set: 96 examples, 3 batches (~16%)\n",
    "* Testing set: 96 examples, 3 batches (~16%)\n",
    "\n",
    "Which adds up to 608 examples and 19 batches.\n",
    "\n",
    "#### Playing Around with the Dataset Wrapper\n",
    "This is our final product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 416\n",
      "Validation set size: 96\n",
      "Testing set size: 96\n"
     ]
    }
   ],
   "source": [
    "from dataset import Patient\n",
    "\n",
    "# Instantiate wrapper\n",
    "patient_dataset = Patient(\"dataset/vcg.npy\", \"dataset/vcg_length.npy\", \"dataset/target.npy\")\n",
    "\n",
    "# Sizes of sets\n",
    "print \"Training set size: \" + str(patient_dataset.train.randomize.max() + 1)\n",
    "print \"Validation set size: \" + str(patient_dataset.validate.randomize.max() + 1)\n",
    "print \"Testing set size: \" + str(patient_dataset.test.randomize.max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of first example: 32\n",
      "Dimensions of VCG: (32, 170, 3)\n",
      "Dimensions of VCG lengths: (32,)\n",
      "Dimensions of targets: (32,)\n"
     ]
    }
   ],
   "source": [
    "# get the first batch\n",
    "batch_vcg, batch_vcg_length, batch_target = patient_dataset.train.next_batch()\n",
    "\n",
    "# Index of first example given in batch\n",
    "print \"Index of first example: \" + str(patient_dataset.train.index)\n",
    "\n",
    "# Shape of next batch\n",
    "print \"Dimensions of VCG: \" + str(batch_vcg.shape)\n",
    "print \"Dimensions of VCG lengths: \" + str(batch_vcg_length.shape)\n",
    "print \"Dimensions of targets: \" + str(batch_target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215 180 397 366 137  43 195 220 279  10 408   3 282 283 256 290 248 119\n",
      " 315 273 121 161  66 377 169 337  37 403 183 340 406 247 203 221 286  63\n",
      "  44 328 200 185 231 399 389 107 131 304 232 338  64 276 378 143 122 135\n",
      " 300 392 277 369 216  93 380  99 324 168 325  41 105 223  74  55 395  38\n",
      " 350 405 182 258 264 228 409 139  90 361 197 318 147 326 151 184 123 321\n",
      " 285  40  96 330 234  92 319 271  57 347 298 303 174 217 156 292 252 243\n",
      " 351 149  20  97  67 394 211 128  77 345 353 360 170 145  21  17 210  26\n",
      " 362  60 357 402 414 173  50 311 126 154 372 177  83 251 175  32  11  59\n",
      " 259  88 370 191  81 225 188 278 323 144 363 214 240 206 178 333  56  75\n",
      " 166 117  79 262 209 287  19  25  48 320 265 297 244 140 192  28 257   0\n",
      " 167 307 386 384 281 125 396 302  76 253 222  84  46 164 114  45  80 233\n",
      " 103 246 219  15  70 186 288 365  72 199 150 133  89 160  22  33   7 241\n",
      " 141  42  52  23 385   5 317 335  14 132 387 383 130  31 205  68 245  62\n",
      "  29  13 236 368 296  82 229 331 356 196   4 224 354 104 346 181  91 305\n",
      " 316 187 153 109  12 112 267  54 165 179 312 371 374  51 275 379 295 332\n",
      " 310 294 255 158 280  73 322 270 235 336 162 263 189  86 299  85 367 415\n",
      "   1 148 376 163 146 212  69 136 172 359 355 237 249 142 308  36 382  39\n",
      " 190 194 155   9 400  87  24 301 254 113 358 134 120 388 373 218 413 393\n",
      " 111 341 293 159 176 118  18 239  65  53  61 171 129  49 138 230  16 127\n",
      " 401 208 260 314  98  94 349   8  47 116 343 269 327 348  34 407 102 411\n",
      " 352 124 157 238  35  58 306 390 410 334 329 375  30 412 242 213 268 291\n",
      " 272 250 152 106 110  95 100 342 313 204 284   6 309 266 101 339 289 227\n",
      " 207 404 201 274 115 261 381  78 193 398  71 391  27   2 198 108 226 364\n",
      " 344 202]\n"
     ]
    }
   ],
   "source": [
    "# We can see the how the set was randomized by accessing the \"randomize\" member\n",
    "print patient_dataset.train.randomize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
